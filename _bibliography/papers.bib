---
---

@article{shen2025llms,
  title    = {Will LLMs Scaling Hit the Wall? Breaking Barriers via Distributed Resources on Massive Edge Devices},
  author   = {Shen, Tao and Zhu, D. and Zhao, Z. and others},
  journal  = {arXiv preprint arXiv:2503.08223},
  year     = {2025},
  selected = {true},
  abbr     = {arXiv},
  abstract = {We explore how distributed resources on massive edge devices can break barriers in LLM scaling, proposing novel approaches for democratized AI infrastructure.},
  arxiv    = {2503.08223},
  preview  = {llm_scaling.png}
}

@inproceedings{shen2024adaptive,
  title={An Adaptive Aggregation Method for Federated Learning via Meta Controller},
  author={Shen, Tao and Li, Z. and Zhao, Z. and others},
  booktitle={Proceedings of the ACM International Conference on Multimedia Asia},
  year={2024},
  selected={true},
  abbr={MM Asia},
  abstract={We propose an adaptive aggregation method for federated learning that uses a meta controller to handle heterogeneous data distributions.},
  preview={federated_meta.png}
}

@article{shen2020federated,
  title={Federated Mutual Learning},
  author={Shen, Tao and Zhang, J. and Jia, X. and others},
  journal={arXiv preprint arXiv:2006.16765},
  year={2020},
  selected={true},
  abbr={arXiv},
  abstract={We introduce federated mutual learning, a novel paradigm that addresses data, model, and objective heterogeneity in distributed learning systems.},
  arxiv={2006.16765},
  preview={federated_mutual.png}
}

@article{shen2018graph,
  title={A Graph-based Power Flow Method for Balanced Distribution Systems},
  author={Shen, Tao and Li, Yanjun and Xiang, Ji},
  journal={Energies},
  volume={11},
  number={3},
  pages={511},
  year={2018},
  publisher={MDPI},
  abbr={Energies},
  abstract={We propose a graph-based direct approach for distribution system power flow without BIBC and BCBV matrices.},
  html={https://www.mdpi.com/1996-1073/11/3/511}
}

@article{zhang2021federated,
  title={Federated Graph Learning--A Position Paper},
  author={Zhang*, Huanding and Shen*, Tao and Wu, Fei and others},
  journal={arXiv preprint arXiv:2105.11099},
  year={2021},
  abbr={arXiv},
  abstract={We present our position on federated graph learning, discussing challenges and opportunities in distributed graph neural networks.},
  arxiv={2105.11099},
  note={*Equal contribution}
}

@article{zhao2025single,
  title={Each Rank Could Be an Expert: Single-ranked Mixture of Experts LoRA for Multi-task Learning},
  author={Zhao, Z. and Zhou, Y. and Zhu, D. and others and Shen, Tao and others},
  journal={arXiv preprint arXiv:2501.15103},
  year={2025},
  abbr={arXiv},
  abstract={We propose a novel approach using single-ranked mixture of experts LoRA for efficient multi-task learning.},
  arxiv={2501.15103}
}

@inproceedings{jiang2025fedcfa,
  title={FedCFA: Alleviating Simpson's Paradox in Model Aggregation with Counterfactual Federated Learning},
  author={Jiang, Z. and Xu, J. and Zhang, S. and others and Shen, Tao and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={17},
  pages={17662},
  year={2025},
  abbr={AAAI},
  abstract={We address Simpson's paradox in federated learning through counterfactual learning approaches.}
}

@article{gao2025flowertune,
  title={FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models},
  author={Gao, Y. and Scamarcia, M.R. and Fernandez-Marques, J. and others and Shen, Tao and others},
  journal={arXiv preprint arXiv:2506.02961},
  year={2025},
  abbr={arXiv},
  abstract={We present FlowerTune, a comprehensive benchmark for evaluating federated fine-tuning of large language models across domains.},
  arxiv={2506.02961}
}

@inproceedings{zhu2025remedy,
  title={Remedy: Recipe Merging Dynamics in Large Vision-Language Models},
  author={Zhu, D. and Song, Y. and Shen, Tao and others},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2025},
  abbr={ICLR},
  abstract={We investigate recipe merging dynamics in large vision-language models and propose novel approaches for model combination.}
}

@article{wu2025knowledge,
  title={Knowledge-empowered, Collaborative, and Co-evolving AI Models: The Post-LLM Roadmap},
  author={Wu, Fei and Shen, Tao and BÃ¤ck, Thomas and others},
  journal={Engineering},
  volume={44},
  pages={87--100},
  year={2025},
  abbr={Engineering},
  abstract={We present a roadmap for post-LLM AI development, focusing on knowledge-empowered and collaborative models.}
}

@article{zhu2024model,
  title={Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models},
  author={Zhu, D. and Sun, Z. and Li, Z. and others and Shen, Tao and others},
  journal={arXiv preprint arXiv:2402.12048},
  year={2024},
  abbr={arXiv},
  abstract={We propose Model Tailor to address catastrophic forgetting in multi-modal large language models.},
  arxiv={2402.12048}
}

@article{zhao2024retrieval,
  title={Retrieval-augmented Mixture of LoRA Experts for Uploadable Machine Learning},
  author={Zhao, Z. and Gan, L. and Wang, G. and others and Shen, Tao and others},
  journal={arXiv preprint arXiv:2406.16989},
  year={2024},
  abbr={arXiv},
  abstract={We present a retrieval-augmented approach for mixture of LoRA experts in uploadable machine learning scenarios.},
  arxiv={2406.16989}
}

@article{zhao2024merging,
  title={Merging LoRAs Like Playing Lego: Pushing the Modularity of LoRA to Extremes through Rank-wise Clustering},
  author={Zhao, Z. and Shen, Tao and Zhu, D. and others},
  journal={arXiv preprint arXiv:2409.16167},
  year={2024},
  abbr={arXiv},
  abstract={We explore extreme modularity in LoRA through rank-wise clustering, enabling flexible model composition.},
  arxiv={2409.16167}
}

@article{li2024training,
  title={Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion},
  author={Li, Z. and Li, Z. and Lin, J. and others and Shen, Tao and others},
  journal={arXiv preprint arXiv:2402.01342},
  year={2024},
  abbr={arXiv},
  abstract={We propose training-time neuron alignment to improve linear mode connectivity and model fusion performance.},
  arxiv={2402.01342}
}

@inproceedings{li2024improving,
  title={Improving Group Connectivity for Generalization of Federated Deep Learning},
  author={Li, Z. and Lin, J. and Li, Z. and others and Shen, Tao and others},
  booktitle={Proceedings of the International Workshop on Federated Foundation Models},
  year={2024},
  abbr={FedFM},
  abstract={We investigate group connectivity improvements for better generalization in federated deep learning.}
}

@article{zhao2024deconfounded,
  title={Deconfounded Hierarchical Multi-granularity Classification},
  author={Zhao, Z. and Gan, L. and Shen, Tao and others},
  journal={Computer Vision and Image Understanding},
  volume={248},
  pages={104108},
  year={2024},
  abbr={CVIU},
  abstract={We propose a deconfounded approach for hierarchical multi-granularity classification tasks.}
}

@article{li2024fedgucci,
  title={FedGuCci: Making Local Models More Connected in Landscape for Federated Learning},
  author={Li, Z. and Lin, J. and Li, Z. and others and Shen, Tao and others},
  journal={arXiv preprint arXiv:2402.18949},
  year={2024},
  abbr={arXiv},
  abstract={We present FedGuCci to improve local model connectivity in federated learning landscapes.},
  arxiv={2402.18949}
}

@article{zhang2023federated,
  title={Federated Unsupervised Representation Learning},
  author={Zhang, F. and Kuang, K. and Chen, L. and others and Shen, Tao and others},
  journal={Frontiers of Information Technology \& Electronic Engineering},
  volume={24},
  number={8},
  pages={1181--1193},
  year={2023},
  abbr={FITEE},
  abstract={We explore federated approaches to unsupervised representation learning across distributed datasets.}
}

@inproceedings{lv2023duet,
  title={Duet: A Tuning-free Device-cloud Collaborative Parameters Generation Framework for Efficient Device Model Generalization},
  author={Lv, Z. and Zhang, W. and Zhang, S. and others and Shen, Tao and others},
  booktitle={Proceedings of the ACM Web Conference},
  pages={3077--3085},
  year={2023},
  abbr={WWW},
  abstract={We propose Duet, a collaborative framework for efficient device-cloud parameter generation without tuning.}
}

@article{yao2022edge,
  title={Edge-cloud Polarization and Collaboration: A Comprehensive Survey for AI},
  author={Yao*, J. and Zhang*, S. and Yao, Y. and others and Shen, Tao and others},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={35},
  number={7},
  pages={6866--6886},
  year={2022},
  abbr={TKDE},
  abstract={We provide a comprehensive survey of edge-cloud polarization and collaboration in AI systems.},
  note={*Equal contribution}
}

@article{zhang2022federated,
  title={FedDTG: Federated Data-Free Knowledge Distillation via Three-Player Generative Adversarial Networks},
  author={Zhang, Z. and Shen, Tao and Zhang, J. and others},
  journal={arXiv preprint arXiv:2201.03169},
  year={2022},
  abbr={arXiv},
  abstract={We propose FedDTG for federated data-free knowledge distillation using three-player GANs.},
  arxiv={2201.03169}
}
